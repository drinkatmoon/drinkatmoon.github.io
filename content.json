{"pages":[],"posts":[{"title":"Airflow简单使用","text":"基本命令使用 1.查看dags: airflow list_dags 2.列出dag下tasks:airflow list_tasks dag_id 3.执行dag下某个task:airflow run dag_id task_id start_time 4.测试整个dag执行:airflow backfill dag_id -s start_time -e end_time -l backfill命令本身是用于回填的，-s表示开始时间，-e表示结束时间,使用该命令可以回填多个DAG Run,最后有个-l参数，表示些任务在本地机器上执行 4.运行整个dag文件:airflow trigger_dag dag_id -r RUN_ID -e EXEC_DATE 4.暂停任务:airflow pause dag_id 4.清楚执行记录:airflow clear -s 开始日期 -e 结束日期 dag_id","link":"/2020/01/09/Airflow%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8/"},{"title":"CDH版hadoop集群扩展节点","text":"在CDH 6.2.0版Hadoop集群上扩展节点需要提前执行cm集群安装过程的前面几步: 扩展前查看集群资源使用率 登录CM管理界面:选择主机,点击add hosts : 选择Add hosts to Cloudera Manager,点继续 输入主机host进行搜索,点击继续 选择存储库,使用自定义存储库(需要提前配置好本地库),点击继续 jdk安装选项(安装过jdk的可不选),点继续 选择ssh登录方式(集群配置过免密访问的可以用在身份认证方法中选择私钥认证) 如果没有使用cdh提供的orace版本的jdk,安装过程会失败,报错如下: 安装cdh提供的orace版本的jdk即可 开始安装cm agent 安装完成,进行主机验证 将该节点加入CDH服务集群 选择未加入CDH集群的主机,点继续 安装parcels包 检查主机正确性 选择主机模板 最后部署客户端配置 然后可以通过CM管理界面在该节点添加角色实例了","link":"/2019/11/09/CDH%E7%89%88hadoop%E9%9B%86%E7%BE%A4%E6%89%A9%E5%B1%95%E8%8A%82%E7%82%B9/"},{"title":"Apache Kylin3.0安装","text":"为了体验apache kylin 3.0在测试环境进行了安装使用:下载地址:http://kylin.apache.org/download/ 1.安装参考官网文档:http://kylin.apache.org/cn/docs/tutorial/kylin_sample.html 12345678910111213141516解压:tar -zxvf apache-kylin-3.0.0-bin-cdh60.tar.gz 配置KYLIN_HOME: export KYLIN_HOME=/home/kylin-3.0.0-cdh60检查安装环境:$KYLIN_HOME/bin/check-env.sh遇到错误:Retrieving hadoop conf dir...Error: Could not find or load main class org.apache.hadoop.hbase.util.GetJavaProperty找到hbase环境目录:ll /etc/alternatives/hbase vi /opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/bin/hbase找到HBASE_CLASSPATH那一行,追加:/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/hbase/lib/*再次执行:$KYLIN_HOME/bin/check-env.sh如果还是报错,暂时忽略export HBASE_CLASSPATH=/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/jars/hbase-server-2.1.0-cdh6.2.0.jar:/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/jars/hbase-common-2.1.0-cdh6.2.0.jar启动异常:Caused by: java.lang.NoClassDefFoundError: org/apache/commons/configuration/ConfigurationExceptioncp /opt/cloudera/cm/common_jars/commons-configuration-1.9.cf57559743f64f0b3a504aba449c9649.jar /home/big-data/kylin-3.0.0-cdh60/tomcat/lib/ 2.运行sh $KYLIN_HOME/bin/kylin.sh start成功启动后日志中会有访问链接:http://slave198:7070/kylin等待1分钟就可以访问了,初始用户名密码为ADMIN/KYLIN(注意是大写)登录进去后看到如下页面:停止kylin:sh $KYLIN_HOME/bin/kylin.sh stop 3.使用demo 3.1运行sh $KYLIN_HOME/bin/sample.sh ,执行成功后,有如下日志: 12Sample cube is created successfully in project &apos;learn_kylin&apos;.Restart Kylin Server or click Web UI =&gt; System Tab =&gt; Reload Metadata to take effect 进入webUI,点击system&gt;&gt;Reload Metadata执行成功后右下角弹出提示框: 然后在菜单栏Choose Project下拉框就能看到新建的项目:选择kylin_sales_cube,点击Antions选择框下的build:选择开始结束日期,点击submit按钮:点击Monitor标签,查看cube build进度,点击最后一列的图标可以查看job详情:等待build完成后,点击insight标签,执行如下sql: 1select part_dt, sum(price) as total_sold, count(distinct seller_id) as sellers from kylin_sales group by part_dt order by part_dt 第一次查询3s完成,之后就是0.1s内完成了:将改sql在hive命令行执行,花了46s: 3.2修改时区: 123kylin.web.timezone以及kylin.stream.event.timezone改为(北京时间)GMT+8点击System标签&gt;&gt;Configuration&gt;&gt;set Config ,输入key:kylin.web.timezone,value:GMT+8,点击update即可 3.3新建project以及Cube点击+按钮，创建新项目,project_name:firstBlood","link":"/2020/01/17/Kylin%E5%AE%89%E8%A3%85/"},{"title":"PostgreSQL客户端安装","text":"centos7安装PG 10客户端:下载地址:https://yum.postgresql.org/rpmchart.php选择版本: 选择groups: 下载客户端包,以及公共包: 上传rpm包到linux环境: 先安装PG依赖包libiu,否则报错:$ yum -y install libicu 在安装libs公共包:$ rpm -ivh postgresql10-libs-10.11-1PGDG.rhel7.x86_64.rpm然后安装client包:$ rpm -ivh postgresql10-10.11-1PGDG.rhel7.x86_64.rpm","link":"/2019/11/15/PostgreSQL%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%AE%89%E8%A3%85/"},{"title":"Airflow安装","text":"1.使用python3 pip安装Airflow pip install apache-airflow ,安装结束提示如下: airflow安装到目录:/usr/local/python3/lib/python3.7/site-packages/airflow/下 2.使用mysql作为airflow的元数据库 创建airflow数据库123create database airflow;grant all on airflow.* to &apos;root&apos;@&apos;%&apos;;flush privileges; 3.配置环境变量vi /etc/profile123export AIRFLOW_HOME=/root/airflowexport SITE_AIRFLOW_HOME=/usr/local/python3/lib/python3.7/site-packages/airflow/export PATH=$PATH:$SITE_AIRFLOW_HOME/bin 生效环境变量 source /etc/profile 4.执行airflow命令初始化操作 添加python依赖,避免初始化失败, 1yum -y install sqlite sqlite-devel 若步骤一完成后，仍然报错： ModuleNotFoundError: No module named ‘_sqlite3’，则是由于python3不兼容_sqlite3,单独安装sqlite3: 12345678910wget https://www.sqlite.org/2019/sqlite-autoconf-3300100.tar.gz --no-check-certificate tar -zxvf sqlite-autoconf-3300100.tar.gz cd sqlite-autoconf-3300100$ mkdir /usr/local/sqlite3$ ./configure --prefix=/usr/local/sqlite3 --disable-static --enable-fts5 --enable-json1 CFLAGS=&quot;-g -O2 -DSQLITE_ENABLE_FTS3=1 -DSQLITE_ENABLE_FTS4=1 -DSQLITE_ENABLE_RTREE=1&quot;$ make &amp;&amp; make install $ cd /root/Python-3.7.6$ LD_RUN_PATH=/usr/local/sqlite3/lib ./configure --prefix=/usr/local/python3 --with-ssl LDFLAGS=&quot;-L /usr/local/sqlite3/lib&quot; CPPFLAGS=&quot; -I /usr/local/sqlite3/include&quot; $ LD_RUN_PATH=/usr/local/sqlite3/lib make$ LD_RUN_PATH=/usr/local/sqlite3/lib make install 验证是否成功,进入python 命令行,import sqlite3,若不报错说明安装成功! 执行airflow,执行结束如报如下错误,可略过 给airflow安装mysql模块,采用mysql作为airflow的元数据库 12345678910111213141516$ pip install &apos;apache-airflow[mysql]&apos; $ yum -y install mysql-devel #完成后验证是否有mysql_config $ find / -name mysql_config $ pip install mysqlclient $ pip install pymysql $ pip install cryptography#避免之后产生错误,需要修改airflow.cfg (默认位于~/airflow/)里的fernet_key#修改方法$ python -c &quot;from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())&quot;#这个命令生成一个key，复制这个key然后替换airflow.cfg文件里的fernet_key的值，$ vi ~/airflow.cfg #修改fernet_keyfernet_key = #修改airflow.cfg文件中的sql_alchemy_conn配置sql_alchemy_conn = mysql+mysqldb://user:passwd@localhost:3306/airflow 初始化airflow db 12345678910$ airflow initdb#如有如下报错 #Global variable explicit_defaults_for_timestamp needs to be on (1) for mysql #修改MySQL配置文件my.cnf #查找my.cnf位置 mysql --help | grep my.cnf #修改my.cnfvi /etc/my.cnf#在[mysqld]下面添加如下配置:explicit_defaults_for_timestamp=true 12345678910 #重启mysql服务使配置生效 $service mysqld restart #检查配置是否生效 mysql&gt; select @@global.explicit_defaults_for_timestamp; +------------------------------------------+| @@global.explicit_defaults_for_timestamp |+------------------------------------------+| 1 |+------------------------------------------+#然后再次执行$ airflow initdb 即可 修改airflow.cfg配置,调整webserver端口,executor,load_examples,以及时区 12345678910111213141516171819202122232425#1.修改webserver端口base_url = http://localhost:8081web_server_port = 8081#2.修改executor#SequentialExecutor是单进程顺序执行任务，默认执行器，通常只用于测试#LocalExecutor是多进程本地执行任务使用的#CeleryExecutor是分布式调度使用（当然也可以单机），生产环境常用#DaskExecutor则用于动态任务调度，常用于数据分析executor = CeleryExecutor#不显示示例DAGload_examples = False#3.修改时区default_timezone = Asia/Shanghai#调整时区还需要修改另外三个文件:#3.1修改webserver页面上右上角展示的时间:vi ${PYTHON_HOME}/lib/python3.7/site-packages/airflow/www/templates/admin/master.html#注释://var UTCseconds = (x.getTime() + x.getTimezoneOffset()*60*1000);var UTCseconds = x.getTime();##修改后#3.2修改webserver lastRun时间,在指定位置添加如下内容，分别找到DagModel(Base),DAG(BaseDag, LoggingMixin)两个类,在get_last_dagrun方法前添加如下方法:vi /usr/local/python3/lib/python3.7/site-packages/airflow/models/dag.pydef utc2local(self,utc): import time epoch = time.mktime(utc.timetuple()) offset = datetime.fromtimestamp(epoch) - datetime.utcfromtimestamp(epoch) return utc + offset 1234567891011121314151617181920#3.3修改dags页面显示时间,根据execution_date定位vi ${PYTHON_HOME}/lib/python3.7/site-packages/airflow/www/templates/airflow/dags.html#将{{ last_run.execution_date.strftime(&quot;%Y-%m-%d %H:%M&quot;) }}修改为:{{ dag.utc2local(last_run.execution_date).strftime(&quot;%Y-%m-%d %H:%M&quot;) }}#将 {{ last_run.start_date.strftime(&quot;%Y-%m-%d %H:%M&quot;) }}修改为:{{ dag.utc2local(last_run.start_date).strftime(&quot;%Y-%m-%d %H:%M&quot;) }}#3.4修改timezone.py文件vi /usr/local/python3/lib/python3.7/site-packages/airflow/utils/timezone.py在utc = pendulum.timezone(&apos;UTC&apos;)下面添加如下内容:from airflow import configuration as conftry: tz = conf.get(&quot;core&quot;, &quot;default_timezone&quot;) if tz == &quot;system&quot;: utc = pendulum.local_timezone() else: utc = pendulum.timezone(tz)except Exception: pass##修改utcnow()函数将d = dt.datetime.utcnow() 修改为d = dt.datetime.now() 123456789101112#3.5修改sqlalchemy.py文件vi /usr/local/python3/lib/python3.7/site-packages/airflow/utils/sqlalchemy.py在utc = pendulum.timezone(&apos;UTC&apos;)下边添加如下代码:from airflow import configuration as conftry: tz = conf.get(&quot;core&quot;, &quot;default_timezone&quot;) if tz == &quot;system&quot;: utc = pendulum.local_timezone() else: utc = pendulum.timezone(tz)except Exception: pass 初始化元数据库 123#初始化元数据库（其实也就是新建airflow依赖的表）$ airflow resetdb#或者使用airflow initdb 5.启动airflow相关组件12345678910111213141516171819202122232425262728#安装如下依赖组件$ pip install celery#安装过程如遇如下Error:#ERROR: No matching distribution found for kombu&lt;4.7,&gt;=4.6.7 (from celery)#需要升级pip版本$ python -m pip install --upgrade pip$ pip install apache-airflow[&apos;kubernetes&apos;]#启动组件：#守护进程运行webserver$ airflow webserver -D#守护进程运行调度器,是一种使用 DAG 定义结合元数据中的任务状态来决定哪些任务需要被执行以及任务执行优先级的过程$ airflow scheduler -D#守护进程运行worker,是实际执行任务逻辑的进程$ airflow worker -D#守护进程Flower Web,建立Celery之上的Web UI，用于监控您的worker,可以不启动$ airflow flower -D####重启webserver和scheduler服务$ ps -ef|egrep &apos;scheduler|airflow-webserver&apos;|grep -v grep|awk &apos;{print $2}&apos;|xargs kill -9$ rm -rf /root/airflow/airflow-webserver.pid &amp;&amp;rm -rf /root/airflow/airflow-scheduler.pid$ airflow webserver -D$ airflow scheduler -D#查看日志,无错误则启动成功$ tail -f /root/airflow/airflow-scheduler.err #重启worker服务ps -ef|egrep &apos;serve_logs|celeryd&apos;|grep -v grep|awk &apos;{print $2}&apos;|xargs kill -9rm -rf /root/airflow/airflow-worker.pidairflow worker -Dtail -f /root/airflow/airflow-worker.err","link":"/2019/12/30/airflow%E5%AE%89%E8%A3%85%E4%BD%BF%E7%94%A8/"},{"title":"azkaban安装","text":"安装azkabanlinux版本:7.4jdk版本:1.8azkaban版本:server目录:/root/azkaban/azkaban-solo-server/build/install/azkaban-solo-server","link":"/2019/12/17/azkaban%E5%AE%89%E8%A3%85/"},{"title":"python2升级python3","text":"系统版本:centos7.5查看版本: cat /etc/redhat-release 1.下载python3安装包下载地址:https://www.python.org/ftp/python/wget https://www.python.org/ftp/python/3.7.6/Python-3.7.6.tar.xz 2.解压安装包xz -d Python-3.7.6.tar.xztar -xf Python-3.7.6.tar 3.在/usr/local路径下创建目录python3mkdir /usr/local/python3 4.编译安装yum install -y libffi-develyum -y install gcc(安装C编译器gcc)cd Python-3.7.6./configure –prefix=/usr/local/python3make all &amp;&amp; make install 5.检查是否安装成功/usr/local/python3/bin/python3.7 -V 6.创建软连接使得系统默认使用python3mv /usr/bin/python /usr/bin/python.bakln -s /usr/local/python3/bin/python3 /usr/bin/pythonmv /usr/bin/pip /usr/bin/pip.bakln -s /usr/local/python3/bin/pip3 /usr/bin/pip 7.验证版本python -Vpip -V 8.升级完python之后，yum就不好用了,需要修改yum源vi /usr/bin/yum#!/usr/bin/python2.7还要修改:vi /usr/libexec/urlgrabber-ext-down","link":"/2019/12/30/centos7%E4%B8%8Apython2%E5%8D%87%E7%BA%A7python3/"},{"title":"flume安装测试","text":"1.下载下载地址：Apache flume 1.9.0 2.部署 创建部署目录，并上传安装包 mkdir sys &amp;&amp; rz -be 解压安装包 tar -zxvf apache-flume-1.9.0-bin.tar.gz 设置环境变量 vi ~/.bash_profile source ~/.bash_profile 修改配置文件 cd $FLUME_HOME/conf mv flume-env.sh.template flume-env.sh vi flume-env.sh,修改JAVA_HOME的值，需要安装java8以上版本；如有需要也可适当调整JAVA_OPTS参数 3.配置 1.实时监控单个日志文件变化，并写入hdfs mv flume-conf.properties.template flume-conf.properties 修改flume-conf,配置source123456789# Name the components on this agenta1.sources = r1a1.sinks = k1a1.channels = c1## configure the source# exec 指的是命令a1.sources.r1.type = execa1.sources.r1.command = tail -F /home/bigdata/logs/test.loga1.sources.r1.channels = c1 修改flume-conf,配置sink12345678910111213141516171819202122232425262728293031# Config the sink#下沉目标a1.sinks.k1.type = hdfsa1.sinks.k1.channel = c1#指定目录, flume帮做目的替换a1.sinks.k1.hdfs.path = hdfs://bi-name1/data/logs/trace_logs/111/%y-%m-%d/#文件的命名, 前缀a1.sinks.k1.hdfs.filePrefix = warn#10 分钟就改目录，生成新的目录 a1.sinks.k1.hdfs.round = truea1.sinks.k1.hdfs.roundValue = 10a1.sinks.k1.hdfs.roundUnit = minute#时间：每3s滚动生成一个新的文件 0表示不使用时间来滚动a1.sinks.k1.hdfs.rollInterval = 0#空间： 文件滚动的大小限制(bytes) 当达到1kb时滚动生成新的文件a1.sinks.k1.hdfs.rollSize = 1024#事件：写入多少个event数据后滚动文件(事件个数)，滚动生成新的文件a1.sinks.k1.hdfs.rollCount = 20#5个事件就开始往里面写入a1.sinks.k1.hdfs.batchSize = 5#用本地时间格式化目录a1.sinks.k1.hdfs.useLocalTimeStamp = true#下沉后, 生成的文件类型，默认是Sequencefile，可用DataStream，则为普通文本a1.sinks.k1.hdfs.fileType = DataStream 修改flume-conf,配置channel12345678# Use a channel which buffers events in memorya1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100# Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1 2.实时监控目录日志文件变化，并写入hdfs 配置source12345678# source类型a1.sources.s1.type = TAILDIR# 元数据位置a1.sources.s1.positionFile = /home/bigdata/file/flume/taildir_position.json# 监控的目录a1.sources.s1.filegroups = f1a1.sources.s1.filegroups.f1=/home/bigdata/logs/.*loga1.sources.s1.fileHeader = true sink和channel配置同上 3.实时监控目录日志文件变化，并写入kafka,配置如下 12345678910111213141516171819202122232425262728# Name the components on this agenta1.sources = r1a1.sinks = k1a1.channels = c1## configure the source# exec 指的是命令a1.sources.r1.type = execa1.sources.r1.command = tail -F /home/bigdata/logs/test.loga1.sources.r1.channels = c1## configure the sinka1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSinka1.sinks.k1.channel=c1a1.sinks.k1.kafka.bootstrap.servers = broker1:9092,broker2:9092,broker3:9092a1.sinks.k1.kafka.topic = test1#指定必须有多少个分区副本接收到了消息，生产者才认为消息发送成功,##0:Never wait,1:wait for leader only,-1:wait for all replicas;default:1a1.sinks.k1.kafka.producer.acks=0## configure the channela1.channels.c1.type = filea1.channels.c1.checkpointDir = /data/flume/checkpointa1.channels.c1.dataDirs = /data/flume/data# Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1 4.运行并测试 启动Flume:$ bin/flume-ng agent -n a1 -c conf -f conf/flume.conf -Dflume.root.logger=INFO,console 使用python2.7追加写入日志文件 1234567891011121314151617181920#-*- coding: utf-8 -*-import timeimport sysimport ioreload(sys)sys.setdefaultencoding(&apos;utf-8&apos;)filePath=&quot;/home/bigdata/logs/test.log&quot;def writeLog(): ff = io.open(filePath,&quot;ab+&quot;) while True: time.sleep(1) tm = str(time.time()) json=&apos;{&quot;name&quot;:&quot;zhangsan&quot;,&quot;age&quot;:20}&apos; ff.write(str(json+&quot;\\n&quot;))if __name__==&quot;__main__&quot;: print(str(time.time())) writeLog() 查看kafka是否收到消息 $ kafka-console-consumer --bootstrap-server slave199:9092 --topic test1 5.bug修复6.补充 python2.7 io.open()的mode参数说明1234567关于open()的mode参数：&apos;r&apos;：读 ; &apos;w&apos;：写 ; &apos;a&apos;：追加&apos;r+&apos; == r+w（可读可写，文件若不存在就报错(IOError)）&apos;w+&apos; == w+r（可读可写，文件若不存在就创建）&apos;a+&apos; ==a+r（可追加可写，文件若不存在就创建）如果是二进制文件，就都加一个b就好啦：&apos;rb&apos; &apos;wb&apos; &apos;ab&apos; &apos;rb+&apos; &apos;wb+&apos; &apos;ab+&apos; Flume启动命令说明1234flume-ng agent --conf conf --conf-file conf/file.log --name agent1 -Dflume.root.logger=DEBUG, console -c (--conf) ： flume的conf文件路径 -f (--conf-file) ： 自定义的flume配置文件 -n (--name)： 自定义的flume配置文件中agent的name flume sink hdfs属性说明:123456type HDFShdfs.path 必填，HDFS 目录路径 (eg hdfs://namenode/flume/webdata/)hdfs.filePrefix FlumeData Flume在目录下创建文件的名称前缀hdfs.fileSuffix – 追加到文件的名称后缀 (eg .avro - 注: 日期时间不会自动添加)hdfs.inUsePrefix – Flume正在处理的文件所加的前缀hdfs.inUseSuffix .tmp Flume正在处理的文件所加的后缀 flume使用指南:http://flume.apache.org/FlumeUserGuide.html","link":"/2019/10/28/flume%E5%AE%89%E8%A3%85%E6%B5%8B%E8%AF%95/"},{"title":"kafka简单使用","text":"kafka版本:2.1.0(基于CDH6.2版本安装)1.创建topic $ kafka-topics --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test1 2.查看topic$ kafka-topics --list --zookeeper localhost:2181 3.向topic中生产消息$ kafka-console-producer --broker-list broker1:9092,broker2:9092,broker:9092 --topic test1 输入消息,然后回车.(注意:有几个broker节点,列几个) 4.消费topic中的消息$ kafka-console-consumer --bootstrap-server localhost:9092 --topic test1 --from-beginning注意:新版本的kafka和之前消费topic的方式有所不同,之前是: kafka-console-consumer –zookeeper localhost:2181 –topic test1 –from-beginnin","link":"/2019/11/01/kafka%E4%BD%BF%E7%94%A8/"},{"title":"mysql基本使用","text":"mysql 5.6 创建索引:ALTER TABLE table_name ADD INDEX index_name(column_name); 修改列类型alter table test modify address char(10) comment ‘’; 修改列名alter table test change column address address1 varchar(30) comment ‘’; 赋权: 12grant all on *.* to &apos;user_name&apos;@&apos;ip&apos; identified by &apos;pwd&apos;; flush privileges; 查看用户所有权限: 1SELECT DISTINCT CONCAT(&apos;User: &apos;&apos;&apos;,user,&apos;&apos;&apos;@&apos;&apos;&apos;,host,&apos;&apos;&apos;;&apos;) AS query FROM mysql.user where user=&apos;user_name&apos; 回收权限: 12revoke all privileges,grant option from &apos;user_name&apos;@&apos;%&apos;;revoke all privileges,grant option from &apos;user_name&apos;@&apos;ip&apos;; 彻底的收权方法:删除mysql的user表中的数据，将没权限访问12delete from mysql.user where user=&apos;root&apos; and host=&apos;%&apos;;flush privileges;","link":"/2019/11/12/mysql%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/"},{"title":"neo4j安装使用","text":"neo4j 安装使用下载地址：http://we-yun.com/index.php/blog/releases-56.html","link":"/2019/11/09/neo4j%E5%AE%89%E8%A3%85%E4%BD%BF%E7%94%A8/"},{"title":"spark streaming消费kafka","text":"Spark Streaming 消费kafka有两种形式： 使用no revivers方式消费kafkaspark版本：2.4.0-cdh6.2.0使用spark streaming消费kafka数据并写入hive中： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129package com.pica.bi.zhuque.streamimport java.sql.Timestampimport java.util.Dateimport com.alibaba.fastjson.{JSON, JSONArray, JSONObject}import org.apache.hadoop.hive.ql.exec.UDFimport org.apache.kafka.clients.consumer.ConsumerRecordimport org.apache.spark.SparkConfimport org.apache.spark.sql.{DataFrame, Row, SparkSession, types}import org.apache.spark.streaming.{Seconds, StreamingContext}import org.apache.kafka.common.serialization.StringDeserializerimport org.apache.spark.rdd.RDDimport org.apache.spark.sql.types.{IntegerType, StringType, StructField, TimestampType}import org.apache.spark.streaming.dstream.InputDStreamimport org.apache.spark.streaming.kafka010.{ConsumerStrategies, KafkaUtils, LocationStrategies}import scala.collection.mutable.{ArrayBuffer, ListBuffer}object ParseLog { val traceFields = List( &quot;id&quot;, &quot;remark1&quot;, &quot;remark2&quot;, &quot;remark3&quot;, &quot;remark4&quot;, &quot;remark5&quot;, &quot;created_time&quot;) def main(args: Array[String]): Unit = { //入口 val spark = SparkSession .builder()// .master(&quot;local[*]&quot;) .appName(&quot;Spark SQL To Hive&quot;) .enableHiveSupport() .getOrCreate() spark.conf.set(&quot;spark.serializer&quot;, &quot;org.apache.spark.serializer.KryoSerializer&quot;) val ssc = new StreamingContext(spark.sparkContext,Seconds(120)) val kafkaParams = Map( &quot;bootstrap.servers&quot; -&gt; &quot;broker1:9092&quot;, &quot;key.deserializer&quot; -&gt; classOf[StringDeserializer], &quot;value.deserializer&quot; -&gt; classOf[StringDeserializer], &quot;group.id&quot; -&gt; &quot;gp1101&quot;, //如果没有记录偏移量,就消费最新的数据 &quot;auto.offset.reset&quot; -&gt; &quot;earliest&quot;, //spark 消费kafka中的偏移量自动维护: kafka 0.10之前的版本自动维护在zookeeper kafka 0.10之后偏移量自动维护topic(__consumer_offsets) //开启自动维护偏移量 &quot;enable.auto.commit&quot; -&gt; (true: java.lang.Boolean) ) val topics = Array(&quot;test1&quot;) //直连方式 val stream: InputDStream[ConsumerRecord[String, String]] = KafkaUtils.createDirectStream[String,String](ssc, LocationStrategies.PreferConsistent, ConsumerStrategies.Subscribe[String,String](topics,kafkaParams)) stream.map(cr =&gt; cr.value()).filter(_.toString().contains(&quot;json:&quot;)).foreachRDD(rs=&gt; { val rdd = rs.flatMap(line=&gt;{ //去掉行首[行尾] var linex = line.toString() if(line.toString().endsWith(&quot;]&quot;) || line.toString().endsWith(&quot;)&quot;)){ linex = line.toString().dropRight(1) } val jsonStr = linex.toString().split(&quot;json:&quot;).apply(1)// println(s&quot;jsonStr:${jsonStr}&quot;) val jsonObj = JSON.parseObject(jsonStr) val lines = new ListBuffer[Row]() if(jsonObj.containsKey(&quot;datas&quot;)){ val jsonArr:JSONArray = jsonObj.getJSONArray(&quot;datas&quot;) val fieldValues = ArrayBuffer[Any]() fieldValues.append(0)//id值默认为0 if(jsonArr.size()&gt;0){ for(i &lt;- 0 to jsonArr.size()-1){ val eachJson:JSONObject = jsonArr.getJSONObject(i) for(field &lt;- traceFields){ if(field.equals(&quot;created_time&quot;)){ fieldValues.append( new Timestamp(new Date().getTime())) }else if(eachJson.containsKey(field)){ fieldValues.append(eachJson.getString(field)) }else{ fieldValues.append(&quot;&quot;) } } lines.append(Row.fromSeq(fieldValues.toSeq)) } } } lines.toList }) val df = createDf(spark,rdd) writeToHive(spark,df) }) ssc.start() ssc.awaitTermination() } def createDf(spark:SparkSession,rdd: RDD[Row]): DataFrame ={ val schemaList = new ListBuffer[StructField] traceFields.map(eachField=&gt;{ var struct:StructField = null if(eachField.equals(&quot;created_time&quot;)){ struct = StructField(eachField, TimestampType, false) }else if(eachField.equals(&quot;id&quot;)){ struct = StructField(eachField, IntegerType, false) }else { struct = StructField(eachField, StringType, false) } schemaList.append(struct) }) val schema = types.StructType(schemaList.toList) println(schema) val resDF = spark.createDataFrame(rdd,schema)// resDF.printSchema()// resDF.show(false) return resDF } def writeToHive(spark: SparkSession, df: DataFrame): Unit = { df.createOrReplaceTempView(&quot;temp_table&quot;) val tday = &quot;2019-11-08&quot; val sql = s&quot;insert into pica_temp.table partition(created_day=&apos;${tday}&apos;) select * from temp_table&quot; println(s&quot;准备执行:${sql}&quot;) spark.sql(sql) }} 补充 编程指南:http://spark.apache.org/docs/latest/streaming-programming-guide.html","link":"/2019/11/01/spark-streaming%E6%B6%88%E8%B4%B9kafka/"},{"title":"翻墙工具搭建","text":"搭建翻墙服务端参考： https://my.oschina.net/u/3219445/blog/1581350","link":"/2019/11/09/%E7%BF%BB%E5%A2%99%E5%B7%A5%E5%85%B7%E6%90%AD%E5%BB%BA/"},{"title":"Apache Atlas2.0安装","text":"下载地址:https://atlas.apache.org/#/Downloads 上传到服务器并解压:tar -zxvf apache-atlas-2.0.0-sources.tar.gz 编译安装 配置maven环境 123456789101112#下载maven安装包$ wget http://mirror.bit.edu.cn/apache/maven/maven-3/3.6.3/binaries/apache-maven-3.6.3-bin.tar.gz$ tar -zxvf apache-maven-3.6.3-bin.tar.gz#修改setting.xml,添加阿里镜像:$ cd apache-maven-3.6.3$ vi conf/settings.xml &lt;mirror&gt; &lt;id&gt;alimaven&lt;/id&gt; &lt;name&gt;aliyun maven&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public/&lt;/url&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;/mirror&gt; 12345678910111213#配置mvn环境变量$ vi /etc/profileexport MAVEN_HOME=/root/apache-maven-3.6.3export PATH=$MAVEN_HOME/bin:$PATH$ source /etc/profile#查看maven版本$ mvn -v#开始编译atlas,将hbase与solr一起进行编译$ cd ~/apache-atlas-sources-2.0.0#2.0 版本已经内部设置 MAVEN_OPTS，可省略该步# export MAVEN_OPTS=&quot;-Xms2g -Xmx2g&quot; $ mvn clean -DskipTests package -Pdist,embedded-hbase-solr#编译完成后如下图,安装包位于distro/target目录下 123456789101112#将target目录下的apache-atlas-2.0.0-bin.tar.gz解压到/opt$ tar -zxvf distro/target/apache-atlas-2.0.0-bin.tar.gz -C /opt$ chown big-data:big-data -R /opt/apache-atlas-2.0.0/$ cd /opt/apache-atlas-2.0.0$ vim conf/atlas-env.shexport JAVA_HOME=/usr/java/jdk1.8.0_181-cloudera/#如果没有使用内嵌的hbase,需要修改conf/atlas-application.properties中相关配置#启动atlas(由于该节点上部署的impalad进程占用了21000端口,需要调整)$ bin/atlas_start.py#停止atlas服务$ bin/atlas_stop.py#通过日志可知atlas启动了内置的hbase,solr 访问atlas123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100启动成功后,浏览器输入:http://slave199:21000 ,默认用户名密码为admin/adminsolr的UI地址:http://slave199:9838/solr运行示例数据:bin/quick_start.pyEnter username for atlas :- adminEnter password for atlas :- admin#过程日志如下:log4j:WARN No such property [maxFileSize] in org.apache.log4j.PatternLayout.log4j:WARN No such property [maxBackupIndex] in org.apache.log4j.PatternLayout.log4j:WARN No such property [maxFileSize] in org.apache.log4j.PatternLayout.log4j:WARN No such property [maxBackupIndex] in org.apache.log4j.PatternLayout.log4j:WARN No such property [maxFileSize] in org.apache.log4j.PatternLayout.log4j:WARN No such property [maxFileSize] in org.apache.log4j.PatternLayout.log4j:WARN No such property [maxBackupIndex] in org.apache.log4j.PatternLayout.Enter username for atlas :- adminEnter password for atlas :- Creating sample types: Created type [DB]Created type [Table]Created type [StorageDesc]Created type [Column]Created type [LoadProcess]Created type [View]Created type [JdbcAccess]Created type [ETL]Created type [Metric]Created type [PII]Created type [Fact]Created type [Dimension]Created type [Log Data]Created type [Table_DB]Created type [View_DB]Created type [View_Tables]Created type [Table_Columns]Created type [Table_StorageDesc]Creating sample entities: Created entity of type [DB], guid: 72bf5b6f-7eb7-4f80-8c7f-7ae7e9b0dbd1Created entity of type [DB], guid: a4392aec-9b36-4d2f-b2d7-2ef2f4a83c10Created entity of type [DB], guid: 0c18f24a-d157-4f85-bd56-c93965371338Created entity of type [Table], guid: d1a30c78-2939-4624-8e17-abf16855e5e0Created entity of type [Table], guid: 47527a26-f176-4bb5-8eb4-57b72c7928b4Created entity of type [Table], guid: 8829432f-932f-43ec-9bab-38603b1bc5dfCreated entity of type [Table], guid: 4faafd4d-2f6d-4046-8356-333837966e1fCreated entity of type [Table], guid: f9562fc1-8eab-4a99-be99-ffc22f632cfdCreated entity of type [Table], guid: 8e4fe39a-48ca-4cd4-8a51-6e359645859eCreated entity of type [Table], guid: 37a931d6-a4d2-439d-9401-668260fb7727Created entity of type [Table], guid: 9e8a05dd-6ec8-4bd4-b42d-3d32f7c28f77Created entity of type [View], guid: dfcf25d4-1081-41ca-9e88-15ec2ad5cce7Created entity of type [View], guid: 3cd7452a-e2ab-495e-abe1-c826975a2757Created entity of type [LoadProcess], guid: fd60947e-4235-4a45-9dd1-31fe702ca360Created entity of type [LoadProcess], guid: 7e45dd12-da78-48c2-911f-88d09c1f3797Created entity of type [LoadProcess], guid: a16c8a80-4c36-40d3-9702-57dbc427ee1dSample DSL Queries: query [from DB] returned [3] rows.query [DB] returned [3] rows.query [DB where name=%22Reporting%22] returned [1] rows.query [DB where name=%22encode_db_name%22] returned [ 0 ] rows.query [Table where name=%2522sales_fact%2522] returned [1] rows.query [DB where name=&quot;Reporting&quot;] returned [1] rows.query [DB where DB.name=&quot;Reporting&quot;] returned [1] rows.query [DB name = &quot;Reporting&quot;] returned [1] rows.query [DB DB.name = &quot;Reporting&quot;] returned [1] rows.query [DB where name=&quot;Reporting&quot; select name, owner] returned [1] rows.query [DB where DB.name=&quot;Reporting&quot; select name, owner] returned [1] rows.query [DB has name] returned [3] rows.query [DB where DB has name] returned [3] rows.query [DB is JdbcAccess] returned [ 0 ] rows.query [from Table] returned [8] rows.query [Table] returned [8] rows.query [Table is Dimension] returned [5] rows.query [Column where Column isa PII] returned [3] rows.query [View is Dimension] returned [2] rows.query [Column select Column.name] returned [10] rows.query [Column select name] returned [9] rows.query [Column where Column.name=&quot;customer_id&quot;] returned [1] rows.query [from Table select Table.name] returned [8] rows.query [DB where (name = &quot;Reporting&quot;)] returned [1] rows.query [DB where DB is JdbcAccess] returned [ 0 ] rows.query [DB where DB has name] returned [3] rows.query [DB as db1 Table where (db1.name = &quot;Reporting&quot;)] returned [ 0 ] rows.query [Dimension] returned [9] rows.query [JdbcAccess] returned [2] rows.query [ETL] returned [6] rows.query [Metric] returned [4] rows.query [PII] returned [3] rows.query [`Log Data`] returned [4] rows.query [Table where name=&quot;sales_fact&quot;, columns] returned [4] rows.query [Table where name=&quot;sales_fact&quot;, columns as column select column.name, column.dataType, column.comment] returned [4] rows.query [from DataSet] returned [10] rows.query [from Process] returned [3] rows.Sample Lineage Info: loadSalesDaily(LoadProcess) -&gt; sales_fact_daily_mv(Table)loadSalesMonthly(LoadProcess) -&gt; sales_fact_monthly_mv(Table)sales_fact(Table) -&gt; loadSalesDaily(LoadProcess)sales_fact_daily_mv(Table) -&gt; loadSalesMonthly(LoadProcess)time_dim(Table) -&gt; loadSalesDaily(LoadProcess)Sample data added to Apache Atlas Server. 配置HA模式Atlas使用说明参考:https://blog.csdn.net/czq850114000/article/details/95463461 引入hive hook 1.修改配置文件 atlas-application.properties 12345######### Hive Hook Configs #########atlas.hook.hive.synchronous=falseatlas.hook.hive.numRetries=3atlas.hook.hive.queueSize=10000atlas.cluster.name=primary 2.将配置文件打包到atlas-plugin-classloader-2.0.0.jar中zip -u /opt/apache-atlas-2.0.0/hook/hive/atlas-plugin-classloader-2.0.0.jar /opt/apache-atlas-2.0.0/conf/atlas-application.properties 3.hive-site.xml以及hive-env.sh,CM可通过界面进行,配置完成后需要重启 1234&lt;property&gt; &lt;name&gt;hive.exec.post.hooks&lt;/name&gt; &lt;value&gt;org.apache.atlas.hive.hook.HiveHook&lt;/value&gt; &lt;/property&gt; HIVE_AUX_JARS_PATH=/opt/apache-atlas-2.0.0/hook/hive 4.将配置文件atlas-application.properties复制到集群hive节点的/etc/hive/conf 目录下sudo cp /opt/apache-atlas-2.0.0/conf/atlas-application.properties /etc/hive/conf/ 5.执行import-hive.sh，用于将Apache Hive数据库和表的元数据导入Apache Atlas，该脚本支持导入特定表，特定数据库中的表或所有数据库和表的元数据：import-hive.sh [-d &lt;database regex&gt; OR --database &lt;database regex&gt;] [-t &lt;table regex&gt; OR --table &lt;table regex&gt;] 123$ export HIVE_HOME=/opt/cloudera/parcels/CDH/lib/hive $ sh bin/import-hive.sh #如果hive库中表很多，执行会花很长时间，可查看import日志： /opt/apache-atlas-2.0.0/logs/application.log ，执行完成后控制台会输出：Hive Meta Data imported successfully!!! 6.查看hive元数据导入结果，点击search功能按钮，在按类型选择下拉选里选择hive_table，点击漏斗图标，弹出属性过滤器，选择名称-包含-pica，点击搜索，结果中显示出了导入元数据的表 7.导入指定数据库表元数据123$ hive -e &quot;create table test_atlas(id int ,name string)&quot;$ sh bin/import-hive.sh -t test_atlas #然后在页面上搜索","link":"/2020/01/20/Apache-Atlas2-0%E5%AE%89%E8%A3%85/"},{"title":"CDH6.2.0 hadoop集群搭建","text":"基于centos7 安装6.2版本CM以及CDH CM安装包下载地址:rpm包:https://archive.cloudera.com/cm6/6.2.0/redhat7/yum/RPMS/x86_64/ allkeys.asc文件:https://archive.cloudera.com/cm6/6.2.0/allkeys.asc CDH安装包下载地址:https://archive.cloudera.com/cdh6/6.2.0/parcels/ 以下操作主要是在root用户下进行》》》 修改hostname（每个节点） 1.1 临时修改 $ hostname hdfs1 1.2 长期修改 $ vi /etc/hostname 配置 /etc/hosts（每个节点） 12345192.168.101.1 hdfs1192.168.101.2 hdfs2192.168.101.3 hdfs3192.168.101.4 hdfs4192.168.101.5 hdfs5 其中选择hdfs1作为主节点 关闭防火墙、禁止防火墙开机自启（每个节点） 123$ systemctl stop firewalld #关闭防火墙$ systemctl disable firewalld #禁止防火墙开机自启$ vi /etc/selinux/config #SELINUX=disabled (修改) 配置ssh免密登录（可以不用配置）登录各节点执行ssh-keygen -t rsa 一路回车到结束，在/root/.ssh/下面会生成一个公钥文件id_rsa.pubcat /home/root/.ssh/id_rsa.pub &gt;&gt; /home/root/.ssh/authorized_keys 将公钥追加到authorized_keys$ chmod 600 /home/root/.ssh/authorized_keys 修改权限将/home/root/.ssh/authorized_keys从当前节点分发到其他各个节点。如：scp /home/root/.ssh/authorized_keys root@hdfs1:/home/root/.ssh/ 配置NTP服务（所有节点）修改时区（改为中国标准时区）$ ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime安装ntp $ yum -y install ntp配置ntp主机服务器，注释掉其他ntp服务器$ vim /etc/ntp.confserver hdfs1 重新启动 ntp服务：$ service ntpd restart设置开机自启：$ systemctl enable ntpd.servicentpdc -c loopinfo #查看与时间同步服务器的时间偏差ntpq -p #查看当前同步的时间服务器ntpstat #查看状态 修改Linux swappiness参数(所有节点）为了避免服务器使用swap功能而影响服务器性能，一般都会把vm.swappiness修改为0（cloudera建议10以下）$ cd /usr/lib/tuned/$ grep &quot;vm.swappiness&quot; * -R #查询出后依次修改 禁用透明页(所有节点）在/etc/rc.local 添加命令：$ echo never &gt; /sys/kernel/mm/transparent_hugepage/defrag$ echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled添加可执行权限：chmod +x /etc/rc.d/rc.local JDK安装（所有节点）$ rpm -qa | grep java # 查询已安装的java$ yum remove java* # 卸载$ rpm -ivh oracle-j2sdk1.8-1.8.0+update181-1.x86_64.rpm$ vi /etc/profile #末尾添加 123export JAVA_HOME=/usr/java/jdk1.8.0_181-cloudera/export CLASSPATH=.:$CLASSPATH:$JAVA_HOME/libexport PATH=$PATH:$JAVA_HOME/bin $ source /etc/profile$ java -version #验证安装是否成功 创建/usr/share/java目录，将mysql-jdbc包放进去（所有节点）$ mkdir -p /usr/share/java$ mv /opt/mysql-j/mysql-connector-java-5.1.34.jar /usr/share/java/注意:mysql-connector-java-5.1.34.jar 一定要命名为mysql-connector-java.jar Mysql安装(选择一个节点) 卸载旧的mysql(如有)以及mariadb：rpm -qa|grep mariadb$ rpm -e --nodeps mariadb-libs-5.5.56-2.el7.x86_64 解压mysql安装包并进行安装: 123456$ tar -xvf mysql-5.7.26-1.el7.x86_64.rpm-bundle.tar$ rpm -ivh mysql-community-common-5.7.26-1.el7.x86_64.rpm $ rpm -ivh mysql-community-libs-5.7.26-1.el7.x86_64.rpm $ rpm -ivh mysql-community-client-5.7.26-1.el7.x86_64.rpm $ rpm -ivh mysql-community-server-5.7.26-1.el7.x86_64.rpm $ rpm -ivh mysql-community-libs-compat-5.7.26-1.el7.x86_64.rpm MYSQL配置: 1234$ mysqld --initialize --user=mysql # 初始化mysql使mysql目录的拥有者为mysql用户$ cat /var/log/mysqld.log # 最后一行会有随机生成的密码 $ systemctl start mysqld.service # 设置mysql服务自启$ mysql -uroot –ppwd 登录进去后,ERROR 1820 (HY000): You must reset your password using ALTER USER statement before executing this statement.需要修改访问密码: ALTER USER USER() IDENTIFIED BY ‘rootpwd’;如果是5.7之前版本,可以使用如下方式: SET PASSWORD = PASSWORD(‘rootpwd’);如果不能登陆,设置免密登录并重启mysql服务:vi /etc/my.cnf #在[mysqld]的段中加上一句：skip-grant-tables ,然后重启mysql： systemctl restart mysqld 创建库(后续安装服务等使用) 12345678910create database metastore default charset utf8 collate utf8_general_ci;create database amon default charset utf8 collate utf8_general_ci;create database rman default charset utf8 collate utf8_general_ci;create database hue default charset utf8 collate utf8_general_ci;create database oozie default charset utf8 collate utf8_general_ci; grant all on metastore.* to &apos;metastore&apos;@&apos;%&apos; identified by &apos;metastore&apos;;grant all on amon.* to &apos;amon&apos;@&apos;%&apos; identified by &apos;amon&apos;;grant all on rman.* to &apos;rman&apos;@&apos;%&apos; identified by &apos;rman&apos;;grant all on hue.* to &apos;hue&apos;@&apos;%&apos; identified by &apos;hue&apos;;grant all on oozie.* to &apos;oozie&apos;@&apos;%&apos; identified by &apos;oozie&apos;; 权限回收:revoke all privileges,grant option from ‘root’@’%’; 安装Httpd服务（hdfs1节点） 1234$ yum install httpd$ service httpd start$ systemctl enable httpd.service 设置httpd服务开机自启$ ps -ef | grep httpd 查看httpd服务是否启动 配置Cloudera Manager包yum源（hdfs1） 配置Web服务器 $ mkdir -p /var/www/html/cloudera-repos/cdh6.2把 CDH 6.2 的三个文件放到该目录下,注意把sha256后缀的文件名修改为sha $ mkdir -p /var/www/html/cloudera-repos/cm6.2将Cloudera Manager安装需要的5个rpm包,asc文件以及jdk文件下载到本地，放在同一目录，执行createrepo命令生成rpm元数据。 123$ cd /var/www/html/cloudera-repos/cm6.2 $ yum install createrepo $ createrepo .#（注意此命令的最后带一个点） 最终 cm6.2目录下多了一个repodata目录 修改 /etc/httpd/conf/httpd.conf 配置文件，在IfModule mime_module中修改以下内容: 把AddType application/x-gzip .gz .tgz 修改为 AddType application/x-gzip .gz .tgz .parcel 重启httpd服务 systemctl restart httpd,并在浏览器访问:http://hdfs1/cloudera-repos/ 制作Cloudera Manager的repo源(hdfs1) $ vi /etc/yum.repos.d/cloudera-manager.repo 1234name=ClouderaManagerbaseurl=http://hdfs1/cloudera-repos/cm6.2enabled=1 # 或者enable=true gpgcheck=0 # 或者gpgcheck=false 更新yum源 123$ yum clean all $ yum makecache $ yum repolist 重启httpd服务 $ systemctl restart httpd 安装Cloudera Manager Server(hdfs1) 安装CM6.2时，没安装jdk的要在各节点先安装官方提供的JDK：oracle-j2sdk1.8-1.8.0+update181-1.x86_64.rpm 第一种方式：因为已经配置好repo仓库所以yum时会到hdfs1/cm6.2目录下找到oracle-j2sdk1.8-1.8.0+update181-1.x86_64.rpm进行安装 $ yum -y install oracle-j2sdk1.8-1.8.0+update181-1.x86_64默认安装在 /usr/java/jdk1.8.0_181-cloudera $ vim /etc/profile 文件中添加如下内容 123export JAVA_HOME=/usr/java/jdk1.8.0_181-clouderaexport CLASSPATH=.:$JAVA_HOME/lib/tools.jarexport PATH=$JAVA_HOME/bin:$PATH 重新加载profile配置文件，让配置文件生效：$ source /etc/profile检查是否已配置好新的JDK：$ java -version然后scp到其他节点. 第二种方式直接使用 rpm -ivh 命令安装 rpm 文件的方式$ rpm -ivh /var/www/html/cm6.2/oracle-j2sdk1.8-1.8.0+update181-1.x86_64.rpm 通过yum安装Cloudera Manager Server：（Cloudera Manager Server的源 已经配置到了repo库中） $ yum -y install cloudera-manager-server cloudera-manager-agent 最后一行显示 Complete!安装完CM后/opt/ 下会出现cloudera目录 $ mv /opt/parcels/* /opt/cloudera/parcel-repo # 将parcel包移动到指定位置 初始化数据库,格式:/opt/cloudera/cm/schema/scm_prepare_database.sh mysql cm cm password /opt/cloudera/cm/schema/scm_prepare_database.sh mysql -hmysql_host -uroot -p'root' --scm-host hdfs1 cm cm_user cm_pwd 注意:如果cm库已经存在,会报错,需要删除 执行成功最后一行显示 All done, your SCM database is configured correctly! 必须保证/usr/share/java目录中已经存在 mysql-connector-java.jar 的软连接：ln -s mysql-connector-java-5.1.46.jar mysql-connector-java.jar 启动Cloudera Manager Server 123$ systemctl start cloudera-scm-server$ ps -ef | grep cloudera-scm-server #查看是否启动$ systemctl status cloudera-scm-server #显示 Active: active (running) 检查端口是否监听 123$ yum install net-tools #安装 netstat$ netstat -lnpt | grep 7180 #要等一段时间启动完全启动成功后，才能看到端口被使用，然后才能真正访问到CM的登录网页tcp 0 0 0.0.0.0:7180 0.0.0.0:* LISTEN 68289/java 通过 hdfs1:7180/cmf/login 访问 CM,之后相关组件可在页面进行安装了 遇到的问题: 在部署agent时出现错误：MainThread agent ERROR Failed to connect to previous supervisor.这时候要确认一下：hostname 和ip是否对应 （/etc/sysconfig/network 和 /etc/hosts ）(hostname 临时修改方法: hostname master)如果对的话，还存在问题，那么尝试用这种方法（杀掉进程）：kill -9 $(pgrep -f supervisord) 然后重启agent即可。 如果在界面安装部署agent过程出现中断导致无法继续安装:需要kill掉agent节点两个进程:12kill -9 ${pgrep -f &apos;cloudera-scm-agent/events&apos;}kill -9 $(pgrep -f supervisord)","link":"/2019/11/09/CDH6-2-0hadoop%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/"},{"title":"DolphinScheduler安装","text":"DolphinScheduler的前身是EasyScheduler,安装包下载地址:https://dolphinscheduler.apache.org/en-us/docs/user_doc/download.html 1.部署后端 1.1后端安装,支持自动安装,源码编译安装,这里使用自动安装 准备,创建新用户big-data,修改用户权限,赋予sudo权限123$ chmod 640 /etc/sudoers &amp;&amp; vim /etc/sudoers #在root ALL=(ALL) ALL行下添加:big-data ALL=(ALL) NOPASSWD: NOPASSWD: ALL 给big-data配置ssh免密(从node1开始,每个节点执行) 123456node1: ssh-keygen -t rsa登录各节点执行ssh-keygen -t rsa 一路回车到结束，在~/.ssh/下面会生成一个公钥文件id_rsa.pubcat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys 将公钥追加到authorized_keys$ chmod 600 ~/.ssh/authorized_keys 修改权限将 ~/.ssh/authorized_keys从当前节点分发到其他各个节点。如：scp ~/.ssh/authorized_keys big-data@hdfs1:~/.ssh/ 创建元数据库: 123CREATE DATABASE escheduler DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;GRANT ALL PRIVILEGES ON escheduler.* TO &apos;escheduler&apos;@&apos;%&apos; IDENTIFIED BY &apos;escheduler&apos;;flush privileges; 创建安装目录: 1234$ sudo mkdir /opt/dolphinScheduler #修改部署目录权限$ chown big-data:big-data /opt/dolphinScheduler$ cd /opt/dolphinScheduler 上传安装包:mkdir escheduler #存放后端包解压文件mkdir front #存放前端包解压文件 修改配置文件:vim /opt/dolphinScheduler/escheduler/conf/application-dao.properties,创建表和导入基础数据注释掉postgre配置,修改mysql配置:spring.datasource.url,username以及password 执行创建表和导入数据脚本:sh backend/script/create-dolphinscheduler.sh执行过程如遇到如下报错:Caused by: java.lang.ClassNotFoundException: com.mysql.jdbc.Driver将mysql驱动拷贝到lib目录下即可: 修改运行参数vi conf/env/.dolphinscheduler_env.sh 修改 install.sh中的各参数，替换成自身业务所需的值 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263# for example postgresql or mysql ...dbtype=&quot;mysql&quot;# db address and portdbhost=&quot;slave199:3306&quot;# db namedbname=&quot;escheduler&quot;# db usernameusername=&quot;escheduler&quot;# db passwprdpassowrd=&quot;escheduler&quot;#各节点部署后端服务目录，和当前解压安装包目录不一样installPath=&quot;/opt/dolphinScheduler/backend&quot;# 部署使用的user，需要有免密sudo权限以及ssh权限deployUser=&quot;big-data&quot;# zk clusterzkQuorum=&quot;master197:2181,slave198:2181,slave199:2181&quot;# 需要部署dolphinscheduler的节点ips=&quot;master197,slave198,slave199&quot;#master节点,可配置多个masters=&quot;slave198&quot;#worker节点workers=&quot;master197,slave198,slave199&quot;#alert服务节点alertServer=&quot;slave198&quot;#api服务节点apiServers=&quot;slave198&quot;# alert config# mail protocolmailProtocol=&quot;SMTP&quot;# mail server hostmailServerHost=&quot;smtp.exmail.qq.com&quot;# mail server portmailServerPort=&quot;465&quot;# sendermailSender=&quot;xxx@qq.com&quot;# usermailUser=&quot;xxx&quot;# sender passwordmailPassword=&quot;passwd&quot;# 资源中心上传选择存储方式：HDFS,S3,NONEresUploadStartupType=&quot;HDFS&quot;# 如果resUploadStartupType为HDFS，defaultFS写namenode地址，支持HA,需要将core-site.xml和hdfs-site.xml放到conf目录下# 如果是S3，则写S3地址，比如说：s3a://escheduler，注意，一定要创建根目录/eschedulerdefaultFS=&quot;hdfs://slave198:8020&quot;# resourcemanager HA configuration, if it is a single resourcemanager, here is yarnHaIps=&quot;&quot;yarnHaIps=&quot;master197,slave199&quot;#如果是单 resourcemanager,只需要配置一个主机名称,如果是resourcemanager HA,则默认配置就好singleYarnIp=&quot;master197&quot;#部署scheduler用户的hdfs根路径hdfsPath=&quot;/big-data&quot;#hdfs集群的root用户hdfsRootUser=&quot;hdfs&quot;# common config# Program root pathprogramPath=&quot;/tmp/dolphinscheduler&quot;# download pathdownloadPath=&quot;/tmp/dolphinscheduler/download&quot;# task execute pathexecPath=&quot;/tmp/dolphinscheduler/exec&quot;# SHELL environmental variable pathshellEnvPath=&quot;$installPath/conf/env/.dolphinscheduler_env.sh&quot;# suffix of the resource fileresSuffixs=&quot;txt,log,sh,conf,cfg,py,java,sql,hql,xml&quot; 将hadoop的配置文件hdfs-site.xml,以及core-site.xml拷贝到conf下: 12cp /etc/hadoop/conf/hdfs-site.xml conf/cp /etc/hadoop/conf/core-site.xml conf/ 执行一键安装部署脚本:sh install.sh dolphinscheduler后端服务启停脚本: 1234 #启动/opt/dolphinScheduler/backend/script/start-all.sh #停止 /opt/dolphinScheduler/backend/script/stop-all.sh 服务成功后,会看到如下相关进程 123456[big-data@slave198 logs]$ jps28817 WorkerServer28995 ApiApplicationServer28756 MasterServer28936 AlertServer28875 LoggerServer 1.2报错解决/opt/dolphinScheduler/backend/logs/*.out日志文件中发现有如下错误: nohup: failed to run command ‘/bin/java’: No such file or directory将JAVA_HOME/bin下的java软连接到/bin下,(所有节点)ln -s $JAVA_HOME/bin/java /bin/java 2.部署前端 2.1下载安装包并解压123cd /opt/dolphinSchedulermkdir fronttar -zxvf apache-dolphinscheduler-incubating-1.2.0-dolphinscheduler-front-bin.tar.gz -C front 2.2执行自动化安装部署脚本sudo sh ./install-dolphinscheduler-ui.sh 使用自动化部署脚本会检查系统环境是否安装了Nginx，如果没有安装则会通过网络自动下载Nginx包安装，通过引导设置后的Nginx配置文件为 /etc/nginx/conf.d/dolphinscheduler.conf 。 2.3报错解决123456789101112131415161718安装nginx过程执行firewall-cmd报错：ModuleNotFoundError: No module named &apos;gi&apos;vim /usr/bin/firewall-cmd 将#！/usr/bin/python -Es 改为 #！/usr/bin/python2 -Esvim /usr/sbin/firewalld, 将#！/usr/bin/python -Es 改为 #！/usr/bin/python2 -Es #然后重启nginx服务即可！systemctl restart nginx#查看nginx服务状态systemctl status nginx#停止nginx服务systemctl stop nginx#如果遇到nginx服务启动失败：#nginx: [error] open() &quot;/run/nginx.pid&quot; failed (2: No such file or directory)#执行： sudo nginx -c /etc/nginx/nginx.conf nginx -s reload#如果报错: [emerg] bind() to 0.0.0.0:80 failed (98: Address already in use)#找到占用80端口的进程，然后kill掉即可！netstat -ntlp | grep 80kill pid 3.开始使用dolphinScheduler 3.1访问：http://slave198:8888/使用dolphinscheduler的默认用户密码登录：admin/dolphinscheduler123 3.2创建一个队列：队列管理&gt;&gt;创建队列，输入名称和队列值，然后提交保存 3.3创建租户：租户管理&gt;&gt;创建租户，输入租户编码，名称，选择队列名称，然后提交 3.4创建普通用户：用户管理&gt;&gt;创建用户，输入用户名称、密码、租户名和邮箱，手机号选填 然后提交 3.5创建告警组：告警组管理&gt;&gt;创建告警组，输入组名称，类型 3.6切换到test用户登录,然后创建一个项目：项目管理&gt;&gt; 3.7给项目创建工作流：点击新建项目进入项目首页，点击工作流定义&gt;&gt;创建工作流&gt;&gt;点击shell图标（选择项有:SHELL、SUB_PROCESS、PROCEDURE、SQL、SPARK、FLINK、MR、PYTHON、DEPENDENT、HTTP），拖拽到画布，新增一个Shell任务，输入节点名称、描述、脚本；选择运行标志，任务优先级 （级别高的任务在执行队列中会优先执行，相同优先级的任务按照先进先出的顺序执行），超时告警，选择超市策略（当任务执行时间超过超时时长可以告警并且超时失败），填写超时时长 ,然后点击确认添加 3.8保存dag：工作流定义完成后，点击保存，输入dag名称，描述，选择租户，最后添加 3.9执行工作流：刚保存的dag处于下线状态，可以编辑但不能执行，需要上线，才能执行，点击操作栏下的上线图标按钮上线，然后点击运行，需要设置运行参数,参数说明如下:1234567失败策略：当某一个任务节点执行失败时，其他并行的任务节点需要执行的策略。”继续“表示：其他任务节点正常执行，”结束“表示：终止所有正在执行的任务，并终止整个流程。通知策略：当流程结束，根据流程状态发送流程执行信息通知邮件。流程优先级：流程运行的优先级，分五个等级：最高（HIGHEST），高(HIGH),中（MEDIUM）,低（LOW），最低（LOWEST）。级别高的流程在执行队列中会优先执行，相同优先级的流程按照先进先出的顺序执行。worker分组： 这个流程只能在指定的机器组里执行。默认是Default，可以在任一worker上执行。通知组： 当流程结束，或者发生容错时，会发送流程信息邮件到通知组里所有成员。收件人：输入邮箱后按回车键保存。当流程结束、发生容错时，会发送告警邮件到收件人列表。抄送人：输入邮箱后按回车键保存。当流程结束、发生容错时，会抄送告警邮件到抄送人列表。 3.10查看执行结果：点击任务实例可以查看每个任务的列表信息，点击操作栏，查看日志信息。 3.11定时执行工作流：点击工作流定义，找到需要定时执行的任务，点击操作栏下定时图标按钮，选择起止时间，定时周期，失败策略，通知策略等，然后点创建，之后可以看到定时状态位下线，点击定时管理跳到定时管理页，点击上线，上线定时任务，定时任务的执行状态可以在操作栏下树形图中快速浏览到 4.使用worker分组执行任务1worker分组，提供了一种让任务在指定的worker上运行的机制。管理员创建worker分组，在任务节点和运行参数中设置中可以指定该任务运行的worker分组，如果指定的分组被删除或者没有指定分组，则该任务会在任一worker上运行。worker分组内多个ip地址（不能写别名），以英文逗号分隔。 4.1切换到admin用户登录，点击安全中心&gt;&gt;worker分组管理&gt;&gt;创建worker分组，输入组名称和ip，多个ip之间用英文,分割。提交后可通过操作栏下编辑按钮进行修改。 5.添加数据源1一般SQL脚本执行时会用到多种数据源，例如MySQL、PostgreSQL、Hive、Impala、Spark、ClickHouse、Oracle、SQL Server，通过添加数据源在DolphinScheduler页面编写Job时直接选择，不用再指定驱动、连接、用户名和密码等信息，可以快速创建一个SQL脚本的工作流Job，同时这个数据源时用户隔离的，每个用户添加的数据源相互独立（admin用户除外，管理员用户可以看到所有用户添加的数据源） 5.1配置mysql数据源，点击数据源中心 &gt;&gt; 创建数据源,输入名称、主机名、端口、用户名密码、以及库名，然后点击测试连接，成功后点击提交即可.","link":"/2020/01/10/DolphinScheduler%E5%AE%89%E8%A3%85/"}],"tags":[{"name":"日志采集","slug":"日志采集","link":"/tags/%E6%97%A5%E5%BF%97%E9%87%87%E9%9B%86/"}],"categories":[{"name":"flume","slug":"flume","link":"/categories/flume/"},{"name":"kafka","slug":"kafka","link":"/categories/kafka/"}]}